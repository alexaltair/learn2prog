1. I want to understand AI.

In order to help prevent AI from being an x-risk, I must have some kind of inside view. I have to understand what kinds will be safe, what kinds are likely to be invented, and what resources are needed for the various kinds.

2. I want to be generally capable.

I need to know how to get things done. I want to do big things, and do them well. I want to be capable of getting things done with groups. I want to be capable of getting things done alone. I want to know when to reach out to others. I want others to reach out to me.

I want to be able to do everything software can do. I want to make software be able to do more.

3. I want some form of stability.

If I'm going to dedicate my mind to reducing x-risk, I can't worry about how to live. I can't spend my time and attention how how to get money, how to be happy, and how to be with people I like. I have to figure it out and then be done with it.